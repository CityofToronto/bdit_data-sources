{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81dce7-5261-414d-bab3-de188ac69eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import requests\n",
    "import datetime\n",
    "from psycopg2 import connect\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_values\n",
    "import logging\n",
    "from time import sleep\n",
    "from airflow.exceptions import AirflowFailException\n",
    "import click\n",
    "CONFIG = configparser.ConfigParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3c63b-17c8-4c35-b735-9957520f4771",
   "metadata": {},
   "source": [
    "## Config [Enter things here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e2297-98b2-4b0d-8c3f-74533aa7d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your .cfg file's path\n",
    "# e.g. CONFIG.read('/home/bqu/db_morbius.cfg')\n",
    "CONFIG.read(' ')\n",
    "dbset = CONFIG['DBSETTINGS']\n",
    "con = connect(**dbset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df7af6-ebdb-4fbc-8de1-fa0806b3756a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07788e-24fd-4aa4-9c36-c90c627c7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The following provides information about the code when it is running and prints out the log messages \n",
    "if they are of logging level equal to or greater than INFO\"\"\"\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede82259-0284-438f-9a0c-12b479d2198a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get mapserver name and generate table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4905567-4aac-4bd8-aa73-138a485bc2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapserver_name(mapserver_n):\n",
    "    \"\"\"\n",
    "    Function to return the mapserver name from integer\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    mapserver_n : numeric\n",
    "        The number of mapserver we will be accessing. 0 for 'cot_geospatial'\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    mapserver_name : string\n",
    "        The name of the mapserver\n",
    "    \"\"\"\n",
    "    \n",
    "    if mapserver_n == 0:\n",
    "        mapserver_name = 'cot_geospatial'\n",
    "    else:\n",
    "        mapserver_name = 'cot_geospatial' + str(mapserver_n)\n",
    "    \n",
    "    return(mapserver_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fe4d8-1d42-48ff-9134-b20cf0d059f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tablename(mapserver, layer_id):\n",
    "    \"\"\"\n",
    "    Function to return the name of the layer\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    mapserver: string\n",
    "        The name of the mapserver we are accessing, returned from function mapserver_name\n",
    "    \n",
    "    layer_id: integer\n",
    "        Unique layer id that represent a single layer in the mapserver\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    output_name\n",
    "        The table name of the layer in database\n",
    "    \"\"\"\n",
    "    \n",
    "    url = 'https://insideto-gis.toronto.ca/arcgis/rest/services/'+mapserver+'/MapServer/layers?f=json'\n",
    "    try:\n",
    "        r = requests.get(url, verify = False, timeout = 20)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err_h:\n",
    "        LOGGER.error(\"Invalid HTTP response: \", err_h)\n",
    "    except requests.exceptions.ConnectionError as err_c:\n",
    "        LOGGER.error(\"Network problem: \", err_c)\n",
    "    except requests.exceptions.Timeout as err_t:\n",
    "        LOGGER.error(\"Timeout: \", err_t)\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        LOGGER.error(\"Error: \", err)\n",
    "    else:\n",
    "        ajson = r.json()\n",
    "        layers = ajson['layers']\n",
    "        for layer in layers:\n",
    "            if layer['id'] == layer_id:\n",
    "                output_name = (layer['name'].lower()).replace(' ', '_')\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621441e0-033c-4cde-8c69-94fc22b70c5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create table in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f189b-0d9e-418d-9826-3ea795aa14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fieldtype(field):\n",
    "    if field == 'esriFieldTypeInteger' or field == 'esriFieldTypeSingle' or field == 'esriFieldTypeInteger' or field=='esriFieldTypeOID' or field == 'esriFieldTypeSmallInteger' or field =='esriFieldGlobalID':\n",
    "        fieldtype = 'integer'\n",
    "    elif field == 'esriFieldTypeString':\n",
    "        fieldtype = 'text'\n",
    "    elif field == 'esriFieldTypeDouble':\n",
    "        fieldtype = 'numeric'\n",
    "    elif field == 'esriFieldTypeDate':\n",
    "        fieldtype = 'timestamp without time zone'\n",
    "    return fieldtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30548ec-a710-4d85-9bf7-1a96f9c7b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audited_table(output_table, return_json, schema_name, primary_key, con):\n",
    "    \"\"\"\n",
    "    Function to create a new table in postgresql for the layer (for audited tables only)\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    output_table : string\n",
    "        Table name for postgresql, returned from function get_tablename\n",
    "\n",
    "    return_json : json\n",
    "        Resulted json response from calling the api, returned from function get_data\n",
    "    \n",
    "    schema_name : string\n",
    "        The schema in which the table will be inserted into\n",
    "        \n",
    "    primary_key : string\n",
    "        Primary key for this layer, returned from dictionary pk_dict\n",
    "    \n",
    "    con: Airflow Connection\n",
    "        Could be the connection to bigdata or to on-prem server\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    insert_columm : SQL composed\n",
    "        Composed object of column name and types use for creating a new postgresql table\n",
    "    \n",
    "    excluded_column : SQL composed\n",
    "        Composed object that is similar to insert_column, but has 'EXCLUDED.' attached before each column name, used for UPSERT query\n",
    "    \"\"\"\n",
    "    \n",
    "    fields = return_json['fields']\n",
    "    insert_column_list = [sql.Identifier((field['name'].lower()).replace('.', '_')) for field in fields]\n",
    "    insert_column_list.append(sql.Identifier('geom'))\n",
    "    insert_column = sql.SQL(',').join(insert_column_list)\n",
    "    \n",
    "    # For audited tables only\n",
    "    excluded_column_list = [sql.SQL('EXCLUDED.') + sql.Identifier((field['name'].lower()).replace('.', '_')) for field in fields]\n",
    "    excluded_column_list.append(sql.SQL('EXCLUDED.') + sql.Identifier('geom'))\n",
    "    excluded_column = sql.SQL(',').join(excluded_column_list)\n",
    "    \n",
    "    # Since this is a temporary table, name it '_table' as opposed to 'table' for now\n",
    "    temp_table_name = '_' + output_table\n",
    "    \n",
    "    with con:\n",
    "        with con.cursor() as cur:\n",
    "            \n",
    "            col_list = [sql.Identifier((field['name'].lower()).replace('.', '_')) + sql.SQL(' ') + sql.SQL(get_fieldtype(field[\"type\"])) for field in fields]\n",
    "            col_list.append(sql.Identifier('geom') + sql.SQL(' ') + sql.SQL('geometry'))\n",
    "            col_list_string = sql.SQL(',').join(col_list)\n",
    "            \n",
    "            LOGGER.info(col_list_string.as_string(con))\n",
    "            create_sql = sql.SQL(\"CREATE TABLE IF NOT EXISTS {schema_table} ({columns})\").format(schema_table = sql.Identifier(schema_name, temp_table_name),\n",
    "                                                                      columns = col_list_string)\n",
    "            LOGGER.info(create_sql.as_string(con))\n",
    "            cur.execute(create_sql)\n",
    "\n",
    "            owner_sql = sql.SQL(\"ALTER TABLE IF EXISTS {schema_table} OWNER to gis_admins\").format(schema_table = sql.Identifier(schema_name, temp_table_name))\n",
    "            cur.execute(owner_sql)\n",
    "    \n",
    "    # Add a pk\n",
    "    with con:\n",
    "        with con.cursor() as cur:\n",
    "            cur.execute(sql.SQL(\"ALTER TABLE {schema_table} ADD PRIMARY KEY ({pk})\").format(schema_table = sql.Identifier(schema_name, temp_table_name),\n",
    "                                                                                               pk = sql.Identifier(primary_key)))\n",
    "    return insert_column, excluded_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775a329-eeb4-4a3c-91a1-fce73d8835ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitioned_table(output_table, return_json, schema_name, con):\n",
    "    \"\"\"\n",
    "    Function to create a new table in postgresql for the layer (for partitioned tables only)\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    output_table : string\n",
    "        Table name for postgresql, returned from function get_tablename\n",
    "\n",
    "    return_json : json\n",
    "        Resulted json response from calling the api, returned from function get_data\n",
    "    \n",
    "    schema_name : string\n",
    "        The schema in which the table will be inserted into\n",
    "    \n",
    "    con: Airflow Connection\n",
    "        Could be the connection to bigdata or to on-prem server\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    insert_columm : SQL composed\n",
    "        Composed object of column name and types use for creating a new postgresql table\n",
    "    \n",
    "    output_table_with_date : string\n",
    "        Table name with date attached at the end, for partitioned tables in postgresql \n",
    "    \"\"\"\n",
    "    \n",
    "    fields = return_json['fields']\n",
    "    insert_column_list = [sql.Identifier((field['name'].lower()).replace('.', '_')) for field in fields]\n",
    "    insert_column_list.insert(0, sql.Identifier('version_date'))\n",
    "    insert_column_list.append(sql.Identifier('geom'))\n",
    "    insert_column = sql.SQL(',').join(insert_column_list)\n",
    "    \n",
    "    # Date format YYYY-MM-DD, for the SQL query\n",
    "    today_string = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    # Date format _YYYYMMDD, to be attached at the end of output_table name\n",
    "    date_attachment = datetime.date.today().strftime('_%Y%m%d')\n",
    "    output_table_with_date = output_table + date_attachment\n",
    "    index_name = output_table_with_date + '_idx'\n",
    "    \n",
    "    with con:\n",
    "        with con.cursor() as cur:\n",
    "            \n",
    "            create_sql = sql.SQL(\"CREATE TABLE IF NOT EXISTS {schema_child_table} PARTITION OF {schema_parent_table} FOR VALUES IN (%s)\").format(schema_child_table = sql.Identifier(schema_name, output_table_with_date),\n",
    "                                                                                                                                            schema_parent_table = sql.Identifier(schema_name, output_table))\n",
    "            cur.execute(create_sql, (today_string, ))\n",
    "\n",
    "            index_sql = sql.SQL(\"CREATE INDEX {idx_name} ON {schema_child_table} USING gist (geom)\").format(idx_name=sql.Identifier(index_name),\n",
    "                                                                                                                schema_child_table=sql.Identifier(schema_name, output_table_with_date))\n",
    "            cur.execute(index_sql)\n",
    "            \n",
    "    return insert_column, output_table_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa7d40-92c0-4a6e-8528-36f37ac26d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometry Switcher \n",
    "def line(geom):\n",
    "    return 'SRID=4326;LineString('+','.join(' '.join(str(x) for x in tup) for tup in geom['paths'][0]) +')'\n",
    "def polygon(geom):\n",
    "    return 'SRID=4326;MultiPolygon((('+','.join(' '.join(str(x) for x in tup) for tup in geom['rings'][0]) +')))'\n",
    "def point(geom):\n",
    "    return 'SRID=4326;Point('+(str(geom['x']))+' '+ (str(geom['y']))+')'  \n",
    "def get_geometry(geometry_type, geom):\n",
    "    switcher = {\n",
    "        'esriGeometryLine':line,\n",
    "        'esriGeometryPolyline': line,\n",
    "        'esriGeometryPoint': point,\n",
    "        'esriGeometryMultiPolygon': polygon,\n",
    "        'esriGeometryPolygon': polygon\n",
    "    }\n",
    "    func = switcher.get(geometry_type)\n",
    "    geometry = (func(geom)) \n",
    "    return geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81928a3a-5710-4e06-a995-32db05be8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_time(input):\n",
    "    \"\"\"\n",
    "    Convert epoch time to postgresql timestamp without time zone\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    input : string\n",
    "        Epoch time attribute in return_json\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    time : string\n",
    "        Time in the type of postgresql timestamp without time zone\n",
    "    \"\"\"\n",
    "    \n",
    "    time = datetime.datetime.fromtimestamp(abs(input)/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131af05a-bace-4c10-81a7-bbf03d08bbf6",
   "metadata": {},
   "source": [
    "## Insert data from ArcGIS to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b12d17-fea1-422b-91e8-02f399cbdc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(mapserver, layer_id, max_number = None, record_max = None):\n",
    "    \"\"\"\n",
    "    Function to retreive layer data from GCCView rest api\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    mapserver : string\n",
    "        The name of the mapserver we are accessing, returned from function mapserver_name\n",
    "\n",
    "    layer_id : integer\n",
    "        Unique layer id that represent a single layer in the mapserver\n",
    "\n",
    "    max_number : integer\n",
    "        Number for parameter `resultOffset` in the query, indicating the number of rows this query is going to skip\n",
    "\n",
    "    record_max : integer\n",
    "        Number for parameter `resultRecordCount` in the query, indicating the number of rows this query is going to fetch\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    return_json : json\n",
    "        Resulted json response from calling the GCCView rest api\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = \"https://insideto-gis.toronto.ca/arcgis/rest/services/{}/MapServer/{}/query\".format(mapserver, layer_id)\n",
    "    \n",
    "    # If the data we want to get is centreline\n",
    "    if mapserver == 'cot_geospatial' and layer_id == 2:\n",
    "        query = {\"where\": \"\\\"FEATURE_CODE_DESC\\\" IN ('Collector','Collector Ramp','Expressway','Expressway Ramp','Local','Major Arterial','Major Arterial Ramp','Minor Arterial','Minor Arterial Ramp','Pending')\",\n",
    "             \"outFields\": \"*\",\n",
    "             \"outSR\": '4326',\n",
    "             \"returnGeometry\": \"true\",\n",
    "             \"returnTrueCurves\": \"false\",\n",
    "             \"returnIdsOnly\": \"false\",\n",
    "             \"returnCountOnly\": \"false\",\n",
    "             \"returnZ\": \"false\",\n",
    "             \"returnM\": \"false\",\n",
    "             \"orderByFields\": \"OBJECTID\", \n",
    "             \"returnDistinctValues\": \"false\",\n",
    "             \"returnExtentsOnly\": \"false\",\n",
    "             \"resultOffset\": \"{}\".format(max_number),\n",
    "             \"resultRecordCount\": \"{}\".format(record_max),\n",
    "             \"f\":\"json\"}\n",
    "    else:\n",
    "        query = {\"where\":\"1=1\",\n",
    "             \"outFields\": \"*\",\n",
    "             \"outSR\": '4326',\n",
    "             \"returnGeometry\": \"true\",\n",
    "             \"returnTrueCurves\": \"false\",\n",
    "             \"returnIdsOnly\": \"false\",\n",
    "             \"returnCountOnly\": \"false\",\n",
    "             \"returnZ\": \"false\",\n",
    "             \"returnM\": \"false\",\n",
    "             \"orderByFields\": \"OBJECTID\", \n",
    "             \"returnDistinctValues\": \"false\",\n",
    "             \"returnExtentsOnly\": \"false\",\n",
    "             \"resultOffset\": \"{}\".format(max_number),\n",
    "             \"resultRecordCount\": \"{}\".format(record_max),\n",
    "             \"f\":\"json\"}\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(base_url, params = query, verify = False, timeout = 300)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as err_h:\n",
    "            LOGGER.error(\"Invalid HTTP response: \", err_h)\n",
    "        except requests.exceptions.ConnectionError as err_c:\n",
    "            LOGGER.error(\"Network problem: \", err_c)\n",
    "            sleep(10)\n",
    "            continue\n",
    "        except requests.exceptions.Timeout as err_t:\n",
    "            LOGGER.error(\"Timeout: \", err_t)\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            LOGGER.error(\"Error: \", err)\n",
    "        else:\n",
    "            return_json = r.json()\n",
    "            break\n",
    "    \n",
    "    return return_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e39cac-10a9-4d42-bbcd-53dda7643374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_limit(return_json):\n",
    "    \"\"\"\n",
    "    Function to check if last query return all rows\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    return_json : json\n",
    "        Resulted json response from calling the api, returned from function get_data\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    keep_adding : Boolean\n",
    "        boolean 'keep_adding' indicating if last query returned all rows in the layer\n",
    "    \"\"\"\n",
    "    \n",
    "    if return_json.get('exceededTransferLimit', False) == True:\n",
    "        keep_adding = True\n",
    "    else:\n",
    "        keep_adding = False\n",
    "    return keep_adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae775756-c82e-4320-b822-13001c4f5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_audited_data(output_table, insert_column, return_json, schema_name, con):\n",
    "    \"\"\"\n",
    "    Function to insert data to our postgresql database, the data is inserted into a temp table (for audited tables)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_table : string\n",
    "        Table name for postgresql, returned from function get_tablename\n",
    "\n",
    "    insert_column : SQL composed\n",
    "        Composed object of column name and types use for creating a new postgresql table\n",
    "\n",
    "    return_json : json\n",
    "        Resulted json response from calling the api, returned from function get_data\n",
    "    \n",
    "    schema_name : string\n",
    "        The schema in which the table will be inserted into\n",
    "    \n",
    "    con: Airflow Connection\n",
    "        Could be the connection to bigdata or to on-prem server\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    features = return_json['features']\n",
    "    fields = return_json['fields']\n",
    "    trials = [[field['name'], field['type']] for field in fields]\n",
    "\n",
    "    for feature in features:\n",
    "        geom = feature['geometry']\n",
    "        geometry_type = return_json['geometryType']\n",
    "        geometry = get_geometry(geometry_type, geom)\n",
    "        \n",
    "        row = []\n",
    "        for trial in trials:\n",
    "            if trial[1] == 'esriFieldTypeDate' and feature['attributes'][trial[0]] != None:\n",
    "                row.append(to_time(feature['attributes'][trial[0]]))\n",
    "            else:\n",
    "                row.append(feature['attributes'][trial[0]])\n",
    "\n",
    "        row.append(geometry)\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Since this is a temporary table, name it '_table' as opposed to 'table' for now (for audited tables)\n",
    "    temp_table_name = '_' + output_table\n",
    "    \n",
    "    insert=sql.SQL(\"INSERT INTO {schema_table} ({columns}) VALUES %s\").format(\n",
    "        schema_table = sql.Identifier(schema_name, temp_table_name), \n",
    "        columns = insert_column\n",
    "    )\n",
    "    with con:\n",
    "        with con.cursor() as cur:\n",
    "               execute_values(cur, insert, rows)\n",
    "    LOGGER.info('Successfully inserted %d records into %s', len(rows), output_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05ccf3-892f-4e0b-9f5b-1d9f81c5177b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_partitioned_data(output_table_with_date, insert_column, return_json, schema_name, con):\n",
    "    \"\"\"\n",
    "    Function to insert data to our postgresql database (for partitioned tables)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_table_with_date : string\n",
    "        Table name for postgresql, returned from function create_partitioned_table\n",
    "\n",
    "    insert_column : SQL composed\n",
    "        Composed object of column name and types use for creating a new postgresql table\n",
    "\n",
    "    return_json : json\n",
    "        Resulted json response from calling the api, returned from function get_data\n",
    "    \n",
    "    schema_name : string\n",
    "        The schema in which the table will be inserted into\n",
    "    \n",
    "    con: Airflow Connection\n",
    "        Could be the connection to bigdata or to on-prem server\n",
    "    \"\"\"   \n",
    "    \n",
    "    today_string = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    rows = []\n",
    "    features = return_json['features']\n",
    "    fields = return_json['fields']\n",
    "    trials = [[field['name'], field['type']] for field in fields]\n",
    "    for feature in features:\n",
    "        geom = feature['geometry']\n",
    "        geometry_type = return_json['geometryType']\n",
    "        geometry = get_geometry(geometry_type, geom)\n",
    "        \n",
    "        row = []\n",
    "        for trial in trials:\n",
    "            if trial[1] == 'esriFieldTypeDate' and feature['attributes'][trial[0]] != None:\n",
    "                row.append(to_time(feature['attributes'][trial[0]]))\n",
    "            else:\n",
    "                row.append(feature['attributes'][trial[0]])\n",
    "\n",
    "        row.insert(0, today_string)\n",
    "        row.append(geometry)\n",
    "        \n",
    "        rows.append(row)\n",
    "\n",
    "    \n",
    "    insert=sql.SQL(\"INSERT INTO {schema_table} ({columns}) VALUES %s\").format(\n",
    "        schema_table = sql.Identifier(schema_name, output_table_with_date), \n",
    "        columns = insert_column\n",
    "    )\n",
    "    with con:\n",
    "        with con.cursor() as cur:\n",
    "               execute_values(cur, insert, rows)\n",
    "    LOGGER.info('Successfully inserted %d records into %s', len(rows), output_table_with_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241c229-ee15-4a89-840c-5f346e1a0dde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Given mapserver_id and layer_id, get their PK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e23a28-5854-4070-8191-e403b40cfbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pk_dict = {\n",
    "\t\"city_ward\": \"area_id\",\n",
    "    \"census_tract\": \"area_id\",\n",
    "    \"neighbourhood_improvement_area\": \"area_id\",\n",
    "    \"priority_neighbourhood_for_investment\": \"area_id\",\n",
    "    \"ibms_district\": \"area_id\",\n",
    "    \"ibms_grid\": \"area_id\",\n",
    "    \"bikeway\": \"centreline_id\",\n",
    "    \"traffic_camera\": \"rec_id\",\n",
    "    \"permit_parking_area\": \"area_long_code\",\n",
    "    \"prai_transit_shelter\": \"id\",\n",
    "    \"traffic_bylaw_point\": \"objectid\",\n",
    "    \"traffic_bylaw_line\": \"objectid\",\n",
    "    \"loop_detector\": \"id\",\n",
    "    \"electrical_vehicle_charging_station\": \"id\",\n",
    "    \"day_care_centre\": \"loc_id\",\n",
    "    \"middle_childcare_centre\": \"id\",\n",
    "    \"business_improvement_area\": \"area_id\",\n",
    "    \"proposed_business_improvement_area\": \"objectid\",\n",
    "    \"film_permit_all\": \"objectid\",\n",
    "    \"film_permit_parking_all\": \"objectid\",\n",
    "    \"hotel\": \"id\",\n",
    "    \"convenience_store\": \"objectid\",\n",
    "    \"supermarket\": \"objectid\",\n",
    "    \"place_of_worship\": \"objectid\",\n",
    "    \"ymca\": \"objectid\",\n",
    "    \"aboriginal_organization\": \"id\",\n",
    "    \"attraction\": \"objectid\",\n",
    "    \"dropin\": \"objectid\",\n",
    "    \"early_years_centre\": \"id\",\n",
    "    \"family_resource_centre\": \"objectid\",\n",
    "    \"food_bank\": \"objectid\",\n",
    "    \"longterm_care\": \"id\",\n",
    "    \"parenting_family_literacy\": \"id\",\n",
    "    \"retirement_home\": \"id\",\n",
    "    \"senior_housing\": \"objectid\",\n",
    "    \"shelter\": \"objectid\",\n",
    "    \"social_housing\": \"objectid\",\n",
    "    \"private_road\": \"objectid\",\n",
    "    \"school\": \"objectid\",\n",
    "    \"library\": \"id\",\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be34a19-228b-4272-86cf-43247a6580b3",
   "metadata": {},
   "source": [
    "## Update audited table (UPSERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e1f5e-47ad-4e69-8945-6f70833c8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table(output_table, insert_column, excluded_column, primary_key, schema_name, con):\n",
    "    \"\"\"\n",
    "    Function to find differences between existing table and the newly created temp table, then UPSERT,\n",
    "    the temp table will be dropped in the end (for audited tables only)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_table : string\n",
    "        Table name for postgresql, returned from function get_tablename\n",
    "\n",
    "    insert_column : SQL composed\n",
    "        Composed object of column name and types use for creating a new postgresql table\n",
    "    \n",
    "    excluded_column : SQL composed\n",
    "        Composed object that is similar to insert_column, but has 'EXCLUDED.' attached before each column name, used for UPSERT query\n",
    "    \n",
    "    primary_key : string\n",
    "        primary key for this layer, returned from dictionary pk_dict\n",
    "    \n",
    "    schema_name : string\n",
    "        The schema in which the table will be inserted into\n",
    "    \n",
    "    con: Airflow Connection\n",
    "        Could be the connection to bigdata or to on-prem server\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    successful_execution : Boolean\n",
    "        whether any error had occured during UPSERT process\n",
    "    \"\"\"\n",
    "\n",
    "    # Boolean to return, whether any error had occured during UPSERT process\n",
    "    successful_execution = True\n",
    "\n",
    "    # Name the temporary table '_table' as opposed to 'table' for now\n",
    "    temp_table_name = '_' + output_table\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    date = (str(now.year)+str(now.month)+str(now.day))\n",
    "    \n",
    "    # Find if old table exists\n",
    "    with con:\n",
    "        with con.cursor() as cur:\n",
    "            \n",
    "            cur.execute(sql.SQL(\"SELECT COUNT(1) FROM information_schema.tables WHERE table_schema = %s AND table_name = %s\"), (schema_name, output_table))\n",
    "            result = cur.fetchone()\n",
    "            # If table exists\n",
    "            if result[0] == 1:\n",
    "            \n",
    "                try:\n",
    "                    # Delete rows that no longer exist in the new table\n",
    "                    cur.execute(sql.SQL(\"DELETE FROM {schema_tablename} WHERE {pk} IN (SELECT {pk} FROM {schema_tablename} EXCEPT SELECT {pk} FROM {schema_temp_table})\").format(\n",
    "                                                                                schema_tablename = sql.Identifier(schema_name, output_table), \n",
    "                                                                                pk = sql.Identifier(primary_key), \n",
    "                                                                                schema_temp_table = sql.Identifier(schema_name, temp_table_name)))\n",
    "\n",
    "                    # And then upsert stuff\n",
    "                    upsert_string = \"INSERT INTO {schema_tablename} ({cols}) SELECT {cols} FROM {schema_temp_table} ON CONFLICT ({pk}) DO UPDATE SET ({cols}) = ({excl_cols}); COMMENT ON TABLE {schema_tablename} IS 'last updated: {date}'\"\n",
    "                    cur.execute(sql.SQL(upsert_string).format(schema_tablename = sql.Identifier(schema_name, output_table),\n",
    "                                                              schema_temp_table = sql.Identifier(schema_name, temp_table_name),\n",
    "                                                              pk = sql.Identifier(primary_key),\n",
    "                                                              cols = insert_column,\n",
    "                                                              excl_cols = excluded_column,\n",
    "                                                              date = sql.Identifier(date)))\n",
    "                    LOGGER.info('Updated table %s', output_table)\n",
    "                except Exception:\n",
    "                    # pass exception to function\n",
    "                    logging.exception(\"Failed to UPSERT\")\n",
    "                    # rollback the previous transaction before starting another\n",
    "                    con.rollback()\n",
    "                    successful_execution = False\n",
    "            \n",
    "            # if table does not exist -> create a new one and add to audit list\n",
    "            else:\n",
    "                try:\n",
    "                    cur.execute(sql.SQL(\"ALTER TABLE {schema_temp_table} RENAME TO {tablename}; COMMENT ON TABLE {schema_tablename} IS 'last updated: {date}'\").format(\n",
    "                                                schema_temp_table = sql.Identifier(schema_name, temp_table_name), \n",
    "                                                schema_tablename = sql.Identifier(schema_name, output_table), \n",
    "                                                date = sql.Identifier(date)))\n",
    "\n",
    "                    \n",
    "                    # Make schema_name and output_table into a single string\n",
    "                    target_audit_table = sql.Literal(schema_name + '.' + output_table)\n",
    "                    cur.execute(sql.SQL(\"SELECT {schema}.audit_table({schematable})\").format(schema = sql.Identifier(schema_name), \n",
    "                                                                                            schematable = target_audit_table))\n",
    "                    LOGGER.info('New table %s created and added to audit table list', output_table)\n",
    "                except Exception:\n",
    "                    # pass exception to function\n",
    "                    logging.exception(\"Failed to create new table\")\n",
    "                    # rollback the previous transaction before starting another\n",
    "                    con.rollback()\n",
    "                    successful_execution = False\n",
    "            \n",
    "            # And then drop the temp table (if exists)\n",
    "            cur.execute(sql.SQL(\"DROP TABLE IF EXISTS {schema_temp_table}\").format(schema_temp_table = sql.Identifier(schema_name, temp_table_name)))\n",
    "    return successful_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5334fbd3-241c-4630-a707-6c0314a99f2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main function that the Task calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146472b-1a30-4599-b510-5900ba22fea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_layer(mapserver_n, layer_id, schema_name, is_audited, cred = None, con = None):\n",
    "    \"\"\"\n",
    "    This function calls to the GCCview rest API and inserts the outputs to the output table in the postgres database.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mapserver : int\n",
    "        The name of the mapserver that host the desired layer\n",
    "\n",
    "    layer_id : int\n",
    "        The id of desired layer\n",
    "    \n",
    "    schema_name : string\n",
    "        The schema in which the table will be inserted into\n",
    "    \n",
    "    is_audited: Boolean\n",
    "        Whether we want to have the table be audited (true) or be partitioned (false)\n",
    "    \n",
    "    cred: Airflow PostgresHook\n",
    "        Contains credentials to enable a connection to a database\n",
    "        Expects a valid cred input when running Airflow DAG\n",
    "    \n",
    "    con: connection to database\n",
    "        Connection object that can connect to a particular database\n",
    "        Expects a valid con object if using command prompt\n",
    "    \"\"\"\n",
    "    successful_task_run = True\n",
    "\n",
    "    # For Airflow DAG\n",
    "    if cred is not None:\n",
    "        con = cred.get_conn()\n",
    "    \n",
    "    # At this point, there should must be a con now\n",
    "    if con is None:\n",
    "        LOGGER.error(\"Unable to establish connection to the database, please pass in a valid con\")\n",
    "        return\n",
    "    \n",
    "    mapserver = mapserver_name(mapserver_n)\n",
    "    output_table = get_tablename(mapserver, layer_id)\n",
    "    #--------------------------------\n",
    "    if is_audited:\n",
    "        primary_key = pk_dict.get(output_table)\n",
    "    #--------------------------------\n",
    "    keep_adding = True\n",
    "    counter = 0\n",
    "    \n",
    "    while keep_adding == True:\n",
    "        \n",
    "        if counter == 0:\n",
    "            return_json = get_data(mapserver, layer_id)\n",
    "            if is_audited:\n",
    "                (insert_column, excluded_column) = create_audited_table(output_table, return_json, schema_name, primary_key, con)\n",
    "            else:\n",
    "                (insert_column, output_table_with_date) = create_partitioned_table(output_table, return_json, schema_name, con)\n",
    "            \n",
    "            features = return_json['features']\n",
    "            record_max=(len(features))\n",
    "            max_number = record_max\n",
    "            \n",
    "            if is_audited:\n",
    "                insert_audited_data(output_table, insert_column, return_json, schema_name, con)\n",
    "            else:\n",
    "                insert_partitioned_data(output_table_with_date, insert_column, return_json, schema_name, con)\n",
    "            \n",
    "            counter += 1\n",
    "            keep_adding = find_limit(return_json)\n",
    "            if keep_adding == False:\n",
    "                LOGGER.info('All records from [mapserver: %s, layerID: %d] have been inserted into %s', mapserver, layer_id, output_table)\n",
    "        else:\n",
    "            return_json = get_data(mapserver, layer_id, max_number = max_number, record_max = record_max)\n",
    "            if is_audited:\n",
    "                insert_audited_data(output_table, insert_column, return_json, schema_name, con)\n",
    "            else:\n",
    "                insert_partitioned_data(output_table_with_date, insert_column, return_json, schema_name, con)\n",
    "            \n",
    "            counter += 1\n",
    "            keep_adding = find_limit(return_json)\n",
    "            if keep_adding == True:\n",
    "                max_number = max_number + record_max\n",
    "            else:\n",
    "                LOGGER.info('All records from [mapserver: %s, layerID: %d] have been inserted into %s', mapserver, layer_id, output_table)\n",
    "    \n",
    "    if is_audited:\n",
    "        successful_task_run = update_table(output_table, insert_column, excluded_column, primary_key, schema_name, con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6dfd9-055a-4fc2-9345-3e1ec71e275e",
   "metadata": {},
   "source": [
    "## Your inputs [Enter things here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf3cfb-8ed7-424b-a0d9-de4f938cd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapserver_n =  # int\n",
    "layer_id =  # int\n",
    "schema_name = ' ' # str\n",
    "is_audited =   # bool\n",
    "get_layer(mapserver_n, layer_id, schema_name, is_audited, con)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
